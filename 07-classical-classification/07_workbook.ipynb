{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Eldave93/machine-learning-workbooks/blob/main/07-classical-classification/07_workbook.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "3HcjpTh9jeci"
   },
   "source": [
    "# Workbook 07 - \"Classical\" Classification\n",
    "by [Dr. David Elliott](https://eldave93.netlify.app/)\n",
    "\n",
    "1. [Workspace Setup](#setup)\n",
    "\n",
    "2. [Problem Understanding](#problem)\n",
    "\n",
    "3. [Data Preparation](#prep)\n",
    "\n",
    "4. [Exploratory Data Analysis](#eda)\n",
    "\n",
    "5. [Data Pre-Processing](#pre)\n",
    "\n",
    "6. [Model Exploration](#explore)\n",
    "\n",
    "7. [Model Refinement](#refine)\n",
    "\n",
    "8. [Summary](#sum)\n",
    "\n",
    "9. [Extra](#extra)\n",
    "\n",
    "__TODO__\n",
    "- Move the KNN explanation into a previous notebook, you could keep the KNN questions though.\n",
    "- Add SVM explanation and questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The aim of this workbook is to start to examine what models we can use to predict categorical variables (e.g. Yes & No)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Workspace Setup\n",
    "Before downloading any data we should think about our workspace. It is assumed if you have made it this far you have already got your workspace setup. There are two ways of using these notebooks. The first is to use Google Colab, which is a website that allows you to write and execute python code through the browser. The second is a local workspace (e.g. Anaconda)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1. Google Colab\n",
    "If you are using google colab then you can follow the below instructions to get setup.\n",
    "\n",
    "First lets check if you are actually using google colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    COLAB=True\n",
    "    \n",
    "    # set the week code\n",
    "    WORKSHOP_NAME = \"07-classical-classification\"\n",
    "except:\n",
    "    COLAB=False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If using colab you will need to install the dependencies and upload the files associated with this workshop to the temporary file store.\n",
    "\n",
    "To do this:\n",
    "1. Download the workbook repository as a .zip file from GitHub (Green \"Code\" button, \"Download ZIP\"),\n",
    "2. On Google Colab click the folder icon on the left panel\n",
    "3. Click the page icon with the upwards arrow on it\n",
    "4. From your local computers file store, upload the .zip file (e.g. `machine-learning-workbooks-main.zip`)\n",
    "\n",
    "__Note__ \n",
    "\n",
    "- Make sure to restart the runtime after installing to ensure everything works correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colab is not being used\n"
     ]
    }
   ],
   "source": [
    "if COLAB:\n",
    "    import os\n",
    "\n",
    "    # check if the environment is already setup to avoid repeating this after \n",
    "    # restarting the runtime\n",
    "    if not os.path.exists(\"machine-learning-workbooks-main\") and os.path.exists(\"machine-learning-workbooks-main.zip\"):\n",
    "          !unzip machine-learning-workbooks-main.zip\n",
    "          \n",
    "    print(\"Setting working directory to:\")\n",
    "    %cd ./machine-learning-workbooks-main/{WORKSHOP_NAME}\n",
    "\n",
    "else:\n",
    "    print(\"Colab is not being used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen above, I automatically set the working directory to be a local version of the workshop repository. This is so all the data, images, and scripts for displaying the solutions works. This is located on the temporary file store associated with this colabs runtime. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2. Local Workspace\n",
    "\n",
    "If your using a local workspace you will need all the packages to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'imblearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\DAVID_~1\\AppData\\Local\\Temp/ipykernel_14328/56507998.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mimblearn\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'imblearn'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import imblearn\n",
    "import sys\n",
    "\n",
    "sys.path.append('../scripts') # add scripts to the path for use later"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you do not already have them the script below, provided `AUTO_INSTALL = True`, will install them for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTO_INSTALL = False\n",
    "\n",
    "if AUTO_INSTALL:\n",
    "    !{sys.executable} -m pip install -r ../scripts/requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3. Displaying solutions\n",
    "\n",
    "The solutions are activated using a new .txt file which can be put in the workshop folder (e.g. `01-end_to_end`). Please put in a request for access.\n",
    "\n",
    "If you have access to the solutions, the following cell will create clickable buttons under each exercise, which will allow you to reveal the solutions.\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- This method was created by [Charlotte Desvages](https://charlottedesvages.com/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/create_widgets.py 07"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# colours for print()\n",
    "class color:\n",
    "   PURPLE = '\\033[95m'\n",
    "   CYAN = '\\033[96m'\n",
    "   DARKCYAN = '\\033[36m'\n",
    "   BLUE = '\\033[94m'\n",
    "   GREEN = '\\033[92m'\n",
    "   YELLOW = '\\033[93m'\n",
    "   RED = '\\033[91m'\n",
    "   BOLD = '\\033[1m'\n",
    "   UNDERLINE = '\\033[4m'\n",
    "   END = '\\033[0m'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 2. Problem Understanding <a id='problem'></a>\n",
    "\n",
    "The data for this week's workshop comes from the Introduction to Statistical Learning textbook. The data for this one is admittedly simulated, but it is useful for demonstrating classification methods.\n",
    "\n",
    "These data are available in `Default.csv` which is provided with this worksheet. \n",
    "\n",
    "__Default:__ Customer default records for a credit card company\n",
    "\n",
    "_\"We are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance.\"_$^5$\n",
    "\n",
    "This data is also a good example of an imballanced problem. Class imbalance is a quite common problem when working with real-world data. This occours when examples from one class or multiple classes are over-represented in a dataset (e.g. spam filtering, fraud \n",
    "detection, disease screening)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "8IBY4qvejecp",
    "outputId": "a2251683-9dec-452b-9c01-c8c3c5a04e92"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Data/Default.csv\", index_col=0)\n",
    "\n",
    "# for now lets just drop the student varible.\n",
    "df = df.drop(\"student\", axis=1)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the data above (which we will imagine comes from a credit card company), we are interested in predicting whether an individual will default on his or her credit card payment, on the basis of annual income and monthly credit card balance.$^5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = \"default\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is going to be a \"supervised learning\" problem, were we learn from the data we have available to us (already labeled for deafults) so in the future we can test new customers to predict if they will default on the loan. For example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the random seed so the same example is selected each time\n",
    "np.random.seed(45)\n",
    "\n",
    "# select a random wine example\n",
    "example = df.sample()\n",
    "\n",
    "# remove our output attribute\n",
    "X_example = example.drop(output, axis=1)\n",
    "\n",
    "X_example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper_functions import create_example_model\n",
    "\n",
    "# Don't worry about understanding this line of code - you will learn how to create your\n",
    "# own `log_pipe` without using `create_example_model` as this is just an example so you\n",
    "# don't need to see the code that went into making this model yet.\n",
    "example_log_pipe = create_example_model(df.drop(example.index[0]), output)\n",
    "\n",
    "# make a prediction with our model\n",
    "example_log_pipe.predict(X_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets check if the owner of this account did default?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example.loc[:,output]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Good! It was able to predict what the person did not default without ever seeing this specific balance and income combination before.\n",
    "\n",
    "Lets have a look at how we can make our own model to do this (essentially what I did inside the function create_example_model)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3bqH-7D7jecq"
   },
   "source": [
    "---\n",
    "\n",
    "# 3. Data Preparation  <a id='prep'></a>\n",
    "\n",
    "Lets start by seeing if we need to do any data cleaning before exploring it further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IMXsAq6fjecr"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 1\n",
    "\n",
    "Examine the structure of the data.\n",
    "- How many observations are in the dataset?\n",
    "- What are the data types?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xF0tpxDzjecr",
    "outputId": "19aa1a45-9a1e-4f42-fe2b-293a2b87f91c"
   },
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZPr9WnGzjecs"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 2\n",
    "\n",
    "Do you observe anything of note from descriptive statistics that may influence our models?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9GiQ8V5qjecs"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 3\n",
    "\n",
    "Are there any missing values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xfh3D_78ject"
   },
   "source": [
    "---\n",
    "\n",
    "Lets now create our feature and output (\"responce\") objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpdbYHyyject",
    "outputId": "e644859c-88b1-46fc-f6da-2d87cea10e1b"
   },
   "outputs": [],
   "source": [
    "data_x, data_y = df.drop(output, axis=1), df[output]\n",
    "\n",
    "print(data_x.shape)\n",
    "print(data_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nDZ3mfV3ject"
   },
   "source": [
    "---\n",
    "\n",
    "At this point we probably should split our training and test sets up (before we start exploring relationships too much!). When splitting our data up we need to account for the class distribution, particularly when it is imballanced. The reason for this is when data is highly imballanced we want our test and validation sets to reflect the distribution of the thing we are trying to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2cUUszMUject",
    "outputId": "c34c0b6c-06bb-4e2a-ec5e-c03cb8bd6aa9"
   },
   "outputs": [],
   "source": [
    "print(color.BOLD+color.UNDERLINE+\"Class Distribution (%)\"+color.END)\n",
    "# get our null accuracy rate \n",
    "data_y.value_counts(normalize=True)*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nhPA7PJajecu"
   },
   "source": [
    "As can be seen above, we have an imballance in our responce varible which we should try keep proportional in our test set. Therefore we can use `stratify = data_y` to ensure this remains the same in the training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ha_9cAPIjecu",
    "outputId": "4f55236a-e511-4b2b-a230-8ccd80c799e0"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "features = list(data_x.columns)\n",
    "X_train, X_test, y_train, y_test = train_test_split(data_x.values, data_y.values, stratify = data_y, \n",
    "                                                    test_size = 0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, stratify = y_train,\n",
    "                                                  test_size = 0.1, random_state=42)\n",
    "\n",
    "print(color.BOLD+color.UNDERLINE+\"Training Distribution\"+color.END)\n",
    "display(pd.Series(y_train).value_counts(normalize=True)*100)\n",
    "print(color.BOLD+color.UNDERLINE+\"Test Distribution\"+color.END)\n",
    "display(pd.Series(y_test).value_counts(normalize=True)*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "80Lu5nAljecu"
   },
   "source": [
    "---\n",
    "\n",
    "# 4. Exploratory Data Analysis <a id='eda'></a>\n",
    "\n",
    "This is quite straight forward data having only two features, but its still worth having a look at how they relate to each other and the outcome variable first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kppOAK1Ijecv"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 4\n",
    "\n",
    "Look at how the variables relate to each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQz97jvojecv"
   },
   "source": [
    "---\n",
    "\n",
    "# 5. Data Pre-Processing <a id='pre'></a>\n",
    "\n",
    "Not really too much we need to do for this data because it is already pretty ready to go... \n",
    "\n",
    "We should could encode output variable to an integer (0,1) for our model to predict like so.\n",
    "\n",
    "__Note__\n",
    "- Some methods with scikit-learn can handle the conversation from strings for you but others can't so best to turn them to 0's and 1's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "qyb3VNhQjecw"
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "LE = LabelEncoder()\n",
    "y_train = LE.fit_transform(y_train)\n",
    "y_val = LE.transform(y_val)\n",
    "y_test = LE.transform(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also some methods of implimenting Logistic regression do not \"require\" scaling of the features, however, due to the implimentation in Scikit-Learn scaling is very much reccomended! In this case as well our features are on quite different scales so it will be a good idea to do this...\n",
    "\n",
    "__Note__\n",
    "- Notice how on the validation and test sets I am only using `transform` - this is because you need to use the mean and standard deviation from the training set!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "SS = StandardScaler()\n",
    "X_train_ = SS.fit_transform(X_train)\n",
    "X_val_ = SS.transform(X_val)\n",
    "X_test_  = SS.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...however as we will be using a `pipeline`, we don't need to worry about doing this yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jOAEZyTijecw"
   },
   "source": [
    "# 6. Model Exploration <a id='explore'></a>\n",
    "\n",
    "For this workbook we will be exploring and refining two different models, Logisitic Regression and K-Nearest-Neighbours. While we are learning about them, lets first reduce the data down to use for examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "\n",
    "# for this example to work I need to ballance the data...more on this\n",
    "# later.\n",
    "df['freq'] = 1./df.groupby('default')['default'].transform('count')\n",
    "example = df.sample(100, weights=\"freq\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1. Logistic Regression\n",
    "\n",
    "Consider this scatter plot showing a sample of people and whether they default or not based on their credit balance. If we were to fit a regression model, what do you think the line might look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.scatterplot(data = example, y = \"default\", x =\"balance\", hue=\"default\", legend=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would look like the regression line below... not super useful as we only want to predict 1 (\"Yes\") or 0 (\"No\")."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y = (example[\"default\"]==\"Yes\").astype(int), \n",
    "            x = example[\"balance\"], ci=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is where logistic regression comes in. Instead of trying to predict a value on a continuous scale, it will try and predict a label through the use of a logistic function. Logistic Regression can only predict binary values (0/1, yes/no, true/false, etc). There are methods we can use it to predict more labels which you may want to look into in the future. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.regplot(y = (example[\"default\"]==\"Yes\").astype(int), \n",
    "            x =example[\"balance\"], logistic=True, ci=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to make a prediction the model assigns each observation a \"probability\" of being 0 or being 1. In otherwords, rather than just being 0 or 1 as above, the model outputs a probability between 0 and 1.\n",
    "\n",
    "Typically we use 50% as the threshold where anything above 50% is assigned a 1 (blue) and below a 0 (orange)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(example[\"balance\"].to_numpy().reshape(-1, 1), example[\"default\"])\n",
    "\n",
    "probs = pd.DataFrame(logreg.predict_proba(example[\"balance\"].to_numpy().reshape(-1, 1)).round(2), \n",
    "                     columns = [\"No\", \"Yes\"])\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.regplot(y = probs[\"Yes\"], x =example[\"balance\"], \n",
    "            logistic=True, ci=None, ax=ax)\n",
    "plt.axline((0, 0.5), slope=0, c=\"black\")\n",
    "\n",
    "plt.fill_between([ax.get_xlim()[0],ax.get_xlim()[1]], -0.1, 0.5, color='orange', alpha=0.2)\n",
    "plt.fill_between([ax.get_xlim()[0],ax.get_xlim()[1]], 0.5, 1.1, color='blue', alpha=0.2)\n",
    "plt.xlim(0,2500)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.ylabel(\"probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So if we want to look at what balance value we would use to predict a default in this case that would be around \\\\$1250. This means that for anyone with a balance under \\\\$1250 this model predicts they defaulted, and anyone higher did. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.regplot(y = probs[\"Yes\"], x =example[\"balance\"], \n",
    "            logistic=True, ci=None, ax=ax)\n",
    "plt.axline((0, 0.5), slope=0, c=\"black\")\n",
    "plt.axline((1250, 0), slope=90, c=\"black\")\n",
    "\n",
    "plt.fill_between([ax.get_xlim()[0],ax.get_xlim()[1]], -0.1, 0.5, color='orange', alpha=0.2)\n",
    "plt.fill_between([ax.get_xlim()[0],ax.get_xlim()[1]], 0.5, 1.1, color='blue', alpha=0.2)\n",
    "plt.xlim(0,2500)\n",
    "plt.ylim(-0.1,1.1)\n",
    "plt.ylabel(\"probability\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets turn back to looking at the \"actual\" default values and not just those that are predicted using the gridlines from above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "sns.scatterplot(y = (example[\"default\"]==\"Yes\").astype(int), x =example[\"balance\"], ax=ax)\n",
    "plt.axline((0, 0.5), slope=0, c=\"black\")\n",
    "plt.axline((1250, 0), slope=90, c=\"black\")\n",
    "plt.fill_between([ax.get_xlim()[0],ax.get_xlim()[1]], -0.1, 0.5, color='orange', alpha=0.2)\n",
    "plt.fill_between([ax.get_xlim()[0],ax.get_xlim()[1]], 0.5, 1.1, color='blue', alpha=0.2)\n",
    "plt.xlim(0,2500)\n",
    "plt.ylim(-0.1,1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is it a perfect model? Clearly not, we can see people who have larger ballances who don't default, but the model will predict that they did (False Positive). Similarly people with smaller ballances, our model will predict they didn't default when they do (False Negative)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.scatterplot(y = (example[\"default\"]==\"Yes\").astype(int), x =example[\"balance\"])\n",
    "plt.axline((0, 0.5), slope=0, c=\"black\")\n",
    "plt.axline((1250, 0), slope=90, c=\"black\")\n",
    "plt.text(300, 0.25, \"True Negative\", fontsize=12)\n",
    "plt.text(300, 0.75, \"False Negative\", fontsize=12)\n",
    "plt.text(1600, 0.25, \"False Positive\", fontsize=12)\n",
    "plt.text(1600, 0.75, \"True Positive\", fontsize=12);\n",
    "\n",
    "plt.fill_between([ax.get_xlim()[0],ax.get_xlim()[1]], -0.1, 0.5, color='orange', alpha=0.2)\n",
    "plt.fill_between([ax.get_xlim()[0],ax.get_xlim()[1]], 0.5, 1.1, color='blue', alpha=0.2)\n",
    "plt.xlim(0,2500)\n",
    "plt.ylim(-0.1,1.1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To improve this model we would have to add more features, like with linear regression. Classification models have bias and variance issues as well which lead to error. They have a range of performance metrics which we will look into later. Similarly to the last workbook though, we can use techniques like train/test split and cross validation to help us assess our models. \n",
    "\n",
    "This is essentially how logistic regression works. Lets recap, for each data point it will calculate the probability of assigning the default (first) label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression()\n",
    "logreg.fit(example[\"balance\"].to_numpy().reshape(-1, 1), example[\"default\"])\n",
    "\n",
    "pd.DataFrame(logreg.predict_proba(example[\"balance\"].to_numpy().reshape(-1, 1)).round(2)[:10], columns = [\"No\", \"Yes\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that probability is lower than a threshold (usually 0.5) then it will assign the other label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg.predict(example[\"balance\"].to_numpy().reshape(-1, 1))[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can change this 0.5 threshold value if you like, although do this carefully. \n",
    "\n",
    "In practice, what is going on behind the scenes is slightly more complicated, but we won’t be covering this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Advantages of logistic regression:__\n",
    "- Highly interpretable (if you remember how).\n",
    "- Model training and prediction are fast.\n",
    "- No tuning is required (excluding regularisation).\n",
    "- Features don't need scaling.\n",
    "- Can perform well with a small number of observations.\n",
    "- Outputs well-calibrated predicted probabilities.\n",
    "\n",
    "__Disadvantages of logistic regression:__\n",
    "- Presumes a linear relationship between the features.\n",
    "- Performance is (generally) not competitive with the best supervised learning methods.\n",
    "- Can't automatically learn feature interactions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hIMqOyC4jecw"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 5\n",
    "\n",
    "Examine and discuss the \"accuracy\" (default scoring method) of the following models (on default parameters) on the data:\n",
    "- `DummyClassifier`: A model that only predicts the class that is the most common.\n",
    "- `LogisticRegression`: See above.\n",
    "\n",
    "What do you notice?\n",
    "\n",
    "__Note__\n",
    "- Using the `.score()` method on classification models defaults to \"Accuracy\".\n",
    "- Like $R^2$ in regression, we want accuracy to be high (close to 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "vCd3Wy2mjecx"
   },
   "source": [
    "---\n",
    "Lets now look at other ways of examining our `LogisticRegression` model's performance other than just \"accuracy\" (which we will look at what that actually means).\n",
    "\n",
    "## 6.2. Performance Metrics\n",
    "\n",
    "A binary classifier can make two types of errors$^5$:\n",
    "- Incorrectly assign an individual __who defaults__ to the __no default__ category (false negative).\n",
    "- Incorrectly assign an individual who __does not default__ to the __default__ category (false positive).\n",
    "\n",
    "Lets use a \"confusion matrix\" to break our models performance down into these these groups (and where it got it right)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "CtFP-8U3jecx",
    "outputId": "158e0438-14c4-4c10-a769-0b9adbf23cbb"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from helper_functions import pretty_confusion_matrix\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn import set_config\n",
    "set_config(display='diagram')\n",
    "\n",
    "log_pipe = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"model\", LogisticRegression(random_state=43))])\n",
    "\n",
    "log_pipe.fit(X_train, y_train)\n",
    "\n",
    "# use the first classifier to predict the validation set    \n",
    "predictions = log_pipe.predict(X_val)\n",
    "\n",
    "# get the confusion matrix as a numpy array\n",
    "confmat = confusion_matrix(y_true=y_val, y_pred=predictions)\n",
    "\n",
    "# use the pretty function to make it nicer\n",
    "pretty_confusion_matrix(confmat, LE.classes_,\n",
    "                        \"Validation Confusion Matrix\",\n",
    "                        labeling=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the overall error rate is low, the error rate among individuals who defaulted is very high. From the perspective of a credit card company that is trying to identify high-risk individuals, this error rate among individuals who default may well be unacceptable<sup>5</sup>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MsXnIZ7ljecz"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 6\n",
    "\n",
    "Using the information above, what would a confusion matrix look like for our dummy model on the validation data? \n",
    "- Check and see if this matches up with your expectation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HV4cgyNFjecz",
    "outputId": "dbedb9c5-7767-4926-d56f-f9d8d022ea9e"
   },
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "4H9aeqcOjecx"
   },
   "source": [
    "---\n",
    "Thinking back to our model, why was our accuracy so high? Lets look how it was calculated...\n",
    "\n",
    "**Error and Accuracy**$^1$\n",
    "\n",
    "Gives general performance information regarding the number of all correct or false predictions comparative to the total number of predictions for both the positive and negative labels.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "ERR &= \\frac{FP+FN}{FP+FN+TP+TN} \\\\ \\\\\n",
    "ACC &= 1-ERR\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- Using `sklearn.metrics` this can be gained using `accuracy_score(y_true=y_val, y_pred=predictions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "zd9yfXfnjecx",
    "outputId": "ab17fd5c-9fe0-428e-8543-352cf4c4c710"
   },
   "outputs": [],
   "source": [
    "FP_i = [0,1]\n",
    "TN_i = [0,0]\n",
    "TP_i = [1,1]\n",
    "FN_i = [1,0]\n",
    "\n",
    "FP = confmat[0,1]\n",
    "TN = confmat[0,0]\n",
    "TP = confmat[1,1]\n",
    "FN = confmat[1,0]\n",
    "\n",
    "ERR = (FP+FN)/(FP+FN+TP+TN)\n",
    "ACC = 1-ERR\n",
    "\n",
    "pretty_confusion_matrix(confmat, LE.classes_, \n",
    "                        \"Accuracy Validation Confusion Matrix\",\n",
    "                        labeling=True, highlight_indexes=[FP_i,FN_i,TP_i,TN_i])\n",
    "\n",
    "print(color.BOLD+'Error (ERR): '+color.END +'%.3f' % ERR)\n",
    "print(color.BOLD+'Accuracy (ACC): '+color.END +'%.3f' % ACC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "D-y5VOrSjecy"
   },
   "source": [
    "So as we can see accuracy only tells part of the story. Lets look at some more.\n",
    "\n",
    "**Precision (PRE)**$^1$\n",
    "\n",
    "- Precision gives information on how precise your model is by looking at how many positive predicted labels are actually positive. \n",
    "- Precision is a good measure to determine, when the costs of a False Positive is high.\n",
    "\n",
    "$$\n",
    "PRE = \\frac{TP}{TP+FP}\n",
    "$$\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- Using `sklearn.metrics` this can be gained using `precision_score(y_true=y_val, y_pred=predictions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "FqKgaUI9jecy",
    "outputId": "fc9b0da1-bc20-44c0-9f8a-bc9f9ed9ee12"
   },
   "outputs": [],
   "source": [
    "PRE = TP/(TP+FP)\n",
    "\n",
    "pretty_confusion_matrix(confmat, LE.classes_, \n",
    "                        \"Precision Validation Confusion Matrix\",\n",
    "                        labeling=True, highlight_indexes=[FP_i,TP_i])\n",
    "\n",
    "print(color.BOLD+'Precision (PRE): '+color.END +'%.3f' % PRE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As this model only predicted \"yes\" when it was \"no\" 3/10 times, our precision seems pretty good... but what about all of those false negatives..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "cFPH02VJjecy"
   },
   "source": [
    "**Recall (or True Positive Rate)**$^1$\n",
    "\n",
    "- Calculates how many of the actual positives our model correctly or incorrectly labelled. \n",
    "\n",
    "- This is useful when the fraction of correctly or misclassified samples in the positive class are of interest.\n",
    "\n",
    "$$\n",
    "REC = \\frac{TP}{FN+TP}\n",
    "$$\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- Using `sklearn.metrics` this can be gained using `recall_score(y_true=y_val, y_pred=predictions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "vt3rlKytjecy",
    "outputId": "143f1448-bc1b-427b-a285-131140d61862"
   },
   "outputs": [],
   "source": [
    "REC = TP/(FN+TP)\n",
    "\n",
    "pretty_confusion_matrix(confmat, LE.classes_, \n",
    "                        \"Recall Validation Confusion Matrix\",\n",
    "                        labeling=True, highlight_indexes=[FN_i,TP_i])\n",
    "\n",
    "print(color.BOLD+'Recall (REC): '+color.END +'%.3f' % REC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This one doesn't look so good and is where our model is having the most issues. It is predicting a lot of defaults and not defaulting!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "tSdhFuuUjecy"
   },
   "source": [
    "**F1-score**$^1$\n",
    "\n",
    "- F1-score is a combination of Recall and Precision. \n",
    "- It is typically used when there is an __uneven class distribution__ due to a large number of True Negatives that you are not as focused on. \n",
    "\n",
    "$$\n",
    "F1 = 2\\left(\\frac{PRE \\times REC}{PRE + REC}\\right)\n",
    "$$\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- Using `sklearn.metrics` this can be gained using `f1_score(y_true=y_val, y_pred=predictions)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "oTZ3kLxijecz",
    "outputId": "fb855f60-65ce-4b65-ba2e-eac40b960a4c"
   },
   "outputs": [],
   "source": [
    "F1 = 2*((PRE*REC)/(PRE+REC))\n",
    "\n",
    "pretty_confusion_matrix(confmat, LE.classes_, \n",
    "                        \"F1-Score Validation Confusion Matrix\",\n",
    "                        labeling=True, highlight_indexes=[FN_i,TP_i, FP_i])\n",
    "\n",
    "print(color.BOLD+'F1-Score (F1): '+color.END +'%.3f' % F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "8q-kKsImjec0"
   },
   "source": [
    "---\n",
    "\n",
    "We can use a classification report, which gives more information such as the macro avg and weighted avg.\n",
    "\n",
    " **Macro Average**\n",
    " - Treats all classes equally as each metric is calculated independently and the average is taken.\n",
    "\n",
    "**Weighted Average** \n",
    "- Each contribution to the average is weighted by the relative number of examples in a class available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "OAtvTQc5jec0"
   },
   "source": [
    "__Notes__\n",
    "\n",
    "- The support is the number of occurrences of each class in y_true.\n",
    "- Notice how the previous metrics were based on the `\"yes\"` class. This is because in binary classification problems, the default positive label is the target (class 1). You can change this if you are more interested in the other classes performance or the average metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "qcklLU8Ljec0",
    "outputId": "3cec8951-25ae-4934-eadf-3bda4eda1872"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "pd.DataFrame(classification_report(y_val, \n",
    "                                   predictions, \n",
    "                                   labels=None, \n",
    "                                   target_names=LE.classes_, \n",
    "                                   sample_weight=None, \n",
    "                                   digits=2, \n",
    "                                   output_dict=True)).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "DQq7EEVbjec0"
   },
   "source": [
    "__Notes__\n",
    "\n",
    "- For further reading on different performance metrics see David M. W. Powers' technical report [Evaluation: From Precision,  Recall and F-Factor to ROC, Informedness, Markedness & Correlation](https://arxiv.org/ftp/arxiv/papers/2010/2010.16071.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6.3. K-Nearest Neighbours (KNN)\n",
    "\n",
    "KNN is a **non-parametric model**, meaning the model is not represented as an equation with parameters (e.g. the $\\beta$ values in linear regression). For parametric models, training data is used to learn a mathematical equation that models the data and once a model is learned, we no longer need the training data. However in the nearest neighbors algorithm, the data is the model as when we encounter a new data sample, we compare it to the training dataset. \n",
    "\n",
    "> You may have heard of the clustering algorithm **k-Means Clustering**. These techniques have nothing in common, aside from both having a parameter $k$!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors classification is (as its name implies) a classification model that uses the $K$ most similar observations in order to make a prediction. K-nearest samples in the training set are compared to the newly encountered sample, and then we use the class labels of the $K$ samples in the training set to assign a label to the new sample.\n",
    "\n",
    "KNN is a supervised learning method; therefore, the training data must have known target values.\n",
    "\n",
    "The process of of prediction using KNN is fairly straightforward:\n",
    "\n",
    "1. Pick a value for K.\n",
    "2. Search for the K observations in the data that are \"nearest\" to the measurements of the unknown value you are trying to predict.\n",
    "    - Euclidian distance is often used as the distance metric, but other metrics are allowed.\n",
    "3. Use the most popular response value from the K \"nearest neighbors\" as the predicted response value for the unknown value.\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- _\"The concept of training doesn't really exist here. Unlike other algorithms, where the training time is dependent on the amount of training data, the computational cost is mostly spent in the nearest neighbors algorithm at prediction time.\"_<sup>1</sup>\n",
    "\n",
    "- _\"Most of the recent research done on the nearest neighbors algorithm is focused on finding the optimum ways to quickly search through the training data during prediction time.\"_<sup>1</sup>\n",
    "\n",
    "---\n",
    "1. Amr, T. (2020). Hands-On Machine Learning with scikit-learn and Scientific Python Toolkits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (14, 8)}) \n",
    "\n",
    "sns.scatterplot(data = example[example['default']==\"No\"], x = \"balance\", y=\"income\", hue=\"default\", palette='Oranges')\n",
    "sns.scatterplot(data = example[example['default']==\"Yes\"], x = \"balance\", y=\"income\",hue=\"default\", palette='Blues')\n",
    "\n",
    "plt.scatter(data = example.iloc[0], x = \"balance\",\n",
    "            y=\"income\", marker=\"+\", c=\"red\", s=400)\n",
    "\n",
    "plt.scatter(data = example.iloc[0], x = \"balance\",\n",
    "            y=\"income\", color = \"darkred\", alpha = 0.3, s=14000)\n",
    "\n",
    "plt.title(\"K=5\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So in the example above we have a new bit of data we want to predict (red cross). We set `k=5` so it looks at the 5 closest points (those inside the red circle). It will add up how many class 0 (\"No\") and how many class 1 (\"Yes\") there are and assign the one with the most (so in this case \"Yes\").\n",
    "\n",
    "---\n",
    "\n",
    "### 🚩 Exercise 7\n",
    "\n",
    "Using the KNN approach on the scatterplot below, visually see what class you would assign the cross to if you were using `k=3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(rc={'figure.figsize': (14, 8)}) \n",
    "\n",
    "sns.scatterplot(data = example[example['default']==\"No\"], x = \"balance\", y=\"income\", hue=\"default\", palette='Oranges')\n",
    "sns.scatterplot(data = example[example['default']==\"Yes\"], x = \"balance\", y=\"income\",hue=\"default\", palette='Blues')\n",
    "\n",
    "plt.scatter(data = example.iloc[59], x = \"balance\",\n",
    "            y=\"income\", marker=\"+\", c=\"red\", s=400);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "It is highly reccomended that for KNN you scale the data before fitting the model. This is because the way \"distance\" is measured is affected by the scale of the values. For example, `balance` ranges between 0 - 2500, but income ranges from 10000 - 60000. \n",
    "\n",
    "Lets look at what happens to how KNN makes a decision if we don't scale our features first. Below is a \"decision boundary\" meaning any new points that land in the orange region will be assigned to \"No\" and any in blue to \"Yes\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "knn_pipe = Pipeline([\n",
    "    #(\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsClassifier())])\n",
    "knn_pipe.fit(example[['balance', 'income']].to_numpy(), \n",
    "             LE.transform(example[['default']].to_numpy()))\n",
    "\n",
    "# Plotting decision regions\n",
    "ax = plot_decision_regions(example[['balance', 'income']].to_numpy(), \n",
    "                      LE.transform(example[['default']].to_numpy()), \n",
    "                      clf=knn_pipe, legend=2, colors='#ff7f0e,#1f77b4')\n",
    "\n",
    "\n",
    "# add legend labels\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles,\n",
    "          [\"No\", \"Yes\"],\n",
    "           framealpha=0.3, scatterpoints=1)\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel(\"balance\")\n",
    "plt.ylabel(\"income\")\n",
    "plt.title(\"k = 5\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks very spikey, and the boundaries don't really look to be where to might expect. This is because the decision on what is closest is being made primarily using income (the y-axis) so the model is only looking at points above or below and not along the x-axis as well. If we scale our features so they are on a similar scale..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "knn_pipe = Pipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"model\", KNeighborsClassifier())])\n",
    "knn_pipe.fit(example[['balance', 'income']].to_numpy(), \n",
    "             LE.transform(example[['default']].to_numpy()))\n",
    "\n",
    "# Plotting decision regions\n",
    "ax = plot_decision_regions(example[['balance', 'income']].to_numpy(), \n",
    "                      LE.transform(example[['default']].to_numpy()), \n",
    "                      clf=knn_pipe, legend=2)\n",
    "\n",
    "# add legend labels\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "ax.legend(handles,\n",
    "          [\"No\", \"Yes\"],\n",
    "           framealpha=0.3, scatterpoints=1)\n",
    "\n",
    "# Adding axes annotations\n",
    "plt.xlabel(\"balance\")\n",
    "plt.ylabel(\"income\")\n",
    "plt.title(\"k = 5\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...It looks a little more like you would expect (uses what is closest using _both_ the x and y axis)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 8\n",
    "\n",
    "Fit a KNN model and assess its performance comparative to the logisitic regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "UeKctYOMjec1"
   },
   "source": [
    "---\n",
    "# 7.  Model Refinement <a id='refine'></a>\n",
    "\n",
    "So far our models are pretty poor. In fact, with only these two bits of information we will find that it is quite difficult to find a model that has high performance across all our metrics (accuracy, precision, recall, ect.) anyway. But what we will find is that in this case we are going to need to build a model that priorises one metric over the others.\n",
    "\n",
    "Lets start by looking at the hyperparameters for logistic regression to see how changing them changes our \"decision boundary\".\n",
    "\n",
    "## 7.1. Model Parameters\n",
    "\n",
    "For our logistic regression model, lets start by looking at the effects of changing the regularization strength ($C$). Regularization is where we apply a penalty to large parameter values in order to reduce overfitting. Essentially, when the model is relying too much on particular datapoints, we penalise it. This reduces the chance our model fits to the noise of our data rather than the underlying structure which will generalise well to new data.\n",
    "\n",
    "Just to confuse things a little, $C$ in Scikit-Learn is the __inverse of regularization strength__, so smaller values specify stronger regularization (therefore penalising the model more for relying on the training data too much)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "AMIeAU0kjec1",
    "outputId": "452f2e08-720a-4536-eb4e-0dfcfa0abd7e"
   },
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "import warnings\n",
    "\n",
    "scatter_kwargs = {'edgecolor': None, 'alpha': 0.7}\n",
    "contourf_kwargs = {'alpha': 0.2}\n",
    "scatter_highlight_kwargs = {'s': 120, 'label': 'Validation data', 'alpha': 0.7}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    fig, axes = plt.subplots(figsize=(15,5), ncols=3)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, c in enumerate([0.01, 0.1, 1]):\n",
    "        \n",
    "        log_pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", LogisticRegression(C = c, random_state=42))])\n",
    "\n",
    "        log_pipe.fit(X_train, y_train)\n",
    "    \n",
    "        plot_decision_regions(X_train, y_train, clf=log_pipe, \n",
    "                              legend=2, X_highlight=X_val[y_val==1],\n",
    "                              contourf_kwargs=contourf_kwargs,\n",
    "                              scatter_kwargs=scatter_kwargs,\n",
    "                              scatter_highlight_kwargs=scatter_highlight_kwargs,\n",
    "                              ax=axes[i])\n",
    "        \n",
    "        axes[i].set_title(\"C = {}\".format(c))\n",
    "        axes[i].set_xlabel(\"balance\")\n",
    "        if i == 0:\n",
    "            axes[i].set_ylabel(\"income\")\n",
    "        \n",
    "    plt.suptitle(\"Logistic Regression Decision Boundaries (Defaulted Validation Data Circled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ibShx-Ljec1"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 9\n",
    "\n",
    "Using the figure above, discuss the effect of $C$ on the decision boundary for the logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "You can also look at decision boundaries for KNN's. They are non-linear and can be prone to overfitting a bit as seen below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_decision_regions\n",
    "\n",
    "scatter_kwargs = {'edgecolor': None, 'alpha': 0.7}\n",
    "contourf_kwargs = {'alpha': 0.2}\n",
    "scatter_highlight_kwargs = {'s': 120, 'label': 'Validation data', 'alpha': 0.7}\n",
    "\n",
    "with warnings.catch_warnings():\n",
    "    warnings.simplefilter(\"ignore\")\n",
    "    \n",
    "    fig, axes = plt.subplots(figsize=(15,5), ncols=3)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    for i, k in enumerate([2, 4, 8]):\n",
    "        \n",
    "        knn_pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", KNeighborsClassifier(n_neighbors = k))])\n",
    "        knn_pipe.fit(X_train, y_train)\n",
    "    \n",
    "        plot_decision_regions(X_train, y_train, clf=knn_pipe, \n",
    "                              legend=2, X_highlight=X_val[y_val==1],\n",
    "                              contourf_kwargs=contourf_kwargs,\n",
    "                              scatter_kwargs=scatter_kwargs,\n",
    "                              scatter_highlight_kwargs=scatter_highlight_kwargs,\n",
    "                              ax=axes[i]\n",
    "                             )\n",
    "        \n",
    "        axes[i].set_title(\"K = {}\".format(k))\n",
    "        \n",
    "    plt.suptitle(\"K-Nearest Neighbour Decision Boundaries (Defaulted Validation Data Circled)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 10\n",
    "\n",
    "Using the figure above, discuss the effect of $K$ on the decision boundary for the KNN model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "d1pcFOjSjec2"
   },
   "source": [
    "---\n",
    "\n",
    "## 7.2. Hyperparameter Tuning\n",
    "\n",
    "$c$ in logistic regression and $k$ in k-nearest neighbours are examples of _\"hyperparameters\"_. The sensitivity to hyperparameter settings change depending on the classifier; for example logistic regression is relatively insensitive to hyperparameter settings. However it is still necessary to find the right range of hyperparameters, otherwise differences between models may just be down to the default tuning parameters rather than the behavior of the model or features.\n",
    "\n",
    "Before we start manually searching over lots of hyperparameters to find our best (or \"optimal\") model, we could look at automating this process using a search method. Two methods are the most common, \"Gridsearch\" and \"Randomsearch\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search\n",
    "\n",
    "A grid search is an exhaustive search paradigm where we specify a list inside a dictionary with various values for different hyperparameters. The computer will use this list to evaluate the model performance for each combination of values to obtain the set that scorers best on the given performance metric<sup>1</sup>.\n",
    "\n",
    "Lets start by setting a range of values for two of logistic regressions parameters (`C` and `class_weight`).\n",
    "\n",
    "__Notes__\n",
    "- The \"`model__`\" part in front of `C` and `class_weight` is there because we will be using a `pipeline`. You need to first specify the name of the pipeline step (`\"model\"`), then `\"__\"`, and then the hyperparameter \"`C`\".\n",
    "\n",
    "1. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a parameter grid with the range of hyperparameteer values of interest\n",
    "PARAM_GRID = {'model__C': [0.5, 1.0, 2.0, 4.0, 8.0], \n",
    "              'model__class_weight': [None, 'balanced']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To search all the possible combinations of the parameters above, it will use each and perform a cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "KF = KFold(n_splits=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to pick a metric (or metrics) to use to examine our model perfomance. Lets use F1 score for now..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer, f1_score\n",
    "\n",
    "SCORER = make_scorer(score_func=f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now lets look at how to apply a gridsearch. I've put it into a function to keep things tidy so take a moment to look over its parts.\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- If the grid param scores were the maximum values evaluated you should try increasing the values to see if the score will improve\n",
    "- Once we've found the optimal hyperparamters we can then actually use these optimal hyper-parameters to re-train our final model on the full training set (including the validation data) to make the model slightly more accurate$^1$. If we set `refit = True` (the default anyway), then this will do this for us for the best estimator.\n",
    "- We'll be using just the training set and later looking how it works on the validation set - granted we could look at combining these again as the cross-validation is creating subsets so potentially makes the validation set redundent... but having a separate validation set helps for later questions...\n",
    "\n",
    "1. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from sklearn.model_selection  import GridSearchCV\n",
    "\n",
    "def Grid_Search(X_train, y_train, estimator, cv, \n",
    "                param_grid, scorer):\n",
    "\n",
    "  # create gridsearch object\n",
    "  gs = GridSearchCV(estimator=estimator,       # model/pipeline to search over\n",
    "                    param_grid=param_grid,     # a dictionary of parameters to search\n",
    "                    scoring=scorer,            # the metric to score them on\n",
    "                    cv=cv,                     # cross-validation method\n",
    "                    return_train_score=True,   # whether to return the training data\n",
    "                    refit = True,              # whether to train a model with the \"best parameters\"\n",
    "                    n_jobs=1)                  # -1 if you want to run in parallel\n",
    "\n",
    "  # fit the gridsearch object\n",
    "  gs = gs.fit(X_train, y_train)\n",
    "\n",
    "  # get the results of the gridsearch\n",
    "  results = pd.DataFrame(gs.cv_results_)\n",
    "\n",
    "  # select the best estimator\n",
    "  clf_pipe = gs.best_estimator_\n",
    "\n",
    "  display(results[['mean_fit_time','mean_score_time',\n",
    "                   'params','mean_test_score',\n",
    "                   'std_test_score']].sort_values(by='mean_test_score',\n",
    "                                                  ascending =False).head())\n",
    "\n",
    "  return clf_pipe, results\n",
    "\n",
    "best_pipe, log_grid_results = Grid_Search(X_train, y_train, log_pipe, \n",
    "                                          KF, PARAM_GRID, SCORER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use stratified k-fold cross-validation, as it can yield better bias and variance estimates in cases of unequal class proportions than RepeatedKFold. This is because \"stratified\" refers to sampling where the classes are separated into \"strata\" and a probability sample is drawn from each group.\n",
    "\n",
    "Its worth noting that some of the folds may not have the same distribution of the classes. This means we could get a validation score that may be a poor estimate of performance (for example we may have a fold with very few positive classes or more than usual). Therefore when doing our gridsearch/randomsearch, we should use a StratifiedKFold to ensure the distribution of classes in our folds reflects the distribution in the larger data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "6-PSlWU0jec2"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "KF = KFold(n_splits=5)\n",
    "SKF = StratifiedKFold(n_splits=5)\n",
    "\n",
    "fold_names = [\"KFold\", \"StratifiedKFold\"]\n",
    "for i, K in enumerate([KF, SKF]):\n",
    "    print(color.BOLD+color.UNDERLINE+fold_names[i]+color.END)\n",
    "    for j, (train_i, test_i) in enumerate(K.split(X_train, y_train)):\n",
    "        fold_no = pd.DataFrame(pd.Series(y_train[test_i]).value_counts(), columns=[\"Fold \"+str(j)])\n",
    "        if j == 0:\n",
    "            fold_nos = fold_no\n",
    "        else:\n",
    "            fold_nos = pd.concat([fold_nos,fold_no], axis=1)\n",
    "        \n",
    "    display(fold_nos)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 11\n",
    "\n",
    "Change the above gridsearch to use a `StratifiedKFold` instead of a normal k-fold.\n",
    "- Does this change what hyperparameters are seen as the \"best\"?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yhoCIzvPjec2"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 12\n",
    "\n",
    "Conduct a gridsearch to find an optimal value for the `k` hyperparameter for a KNN model. Can you find a model that has improved performance vs. default settings?\n",
    "\n",
    "__Notes__\n",
    "- The `scoring` parameter can take multiple values so you can assess different performance metrics (e.g. `scoring = [\"accuracy\", \"f1\",\"recall\",\"precision\"]`). If you do this and want the search cv to output a fitted model, you need to specify what metric to use as the one to find the \"best parameters\" (e.g. `refit=\"accuracy\"`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Random Search\n",
    "\n",
    "Although a powerful method, gridsearch is computationally expensive as it has to evaluate all the possible parameter combinations. If we were working with a larger dataset or wanting to search lots of hyperparameter options, it is much better to randomly sample different random parameter combinations within a given range using RandomizedSearchCV<sup>1</sup>.\n",
    "\n",
    "You can provide distributions for sampling parameters as demonstrated for $c$ below. The distributions just make it more likely a value will be picked within a certain range. For example an exponential continuous random variable could be set so its more likely to get a lower number. Lets plot 1000 random samples and see where they lie.\n",
    "\n",
    "1. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import expon\n",
    "\n",
    "plt.hist(expon(scale=1).rvs(size=1000), histtype='stepfilled')\n",
    "plt.xlabel('Parameter Number')\n",
    "plt.ylabel('Times picked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...or you could use a gamma continuous random variable to provide a more 'normal' distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import gamma\n",
    "\n",
    "plt.hist(gamma(a=10, scale=1).rvs(size=1000), histtype='stepfilled')\n",
    "plt.xlabel('Parameter Number')\n",
    "plt.ylabel('Times picked')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So instead of a set parameter dictionary lets provide our C with a random number between 0.01 and 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "85ckjDn8jec2",
    "outputId": "90c75e77-6fad-4986-a13f-317eaa3e5fd4"
   },
   "outputs": [],
   "source": [
    "from scipy.stats import uniform\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "log_pipe = Pipeline([\n",
    "            (\"scaler\", StandardScaler()),\n",
    "            (\"model\", LogisticRegression(C = c, random_state=42))])\n",
    "    \n",
    "# specify parameters and distributions to sample from\n",
    "log_param_dist = {'model__C':uniform(0.01, 1),\n",
    "                  'model__class_weight': [None, 'balanced']\n",
    "                 }\n",
    "\n",
    "log_rs = RandomizedSearchCV(log_pipe, \n",
    "                            param_distributions=log_param_dist,\n",
    "                            n_iter=30, \n",
    "                            scoring = [\"accuracy\", \"f1\",\"recall\",\"precision\"], \n",
    "                            cv=StratifiedKFold(n_splits=5),\n",
    "                            refit=\"accuracy\", \n",
    "                            random_state=42,\n",
    "                            return_train_score=True)\n",
    "\n",
    "log_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U5ojqM1Sjec3",
    "outputId": "04cc0e8f-b49b-458a-d17f-4b060225cab2"
   },
   "outputs": [],
   "source": [
    "log_rs_df = pd.DataFrame(log_rs.cv_results_)\n",
    "log_rs_df.sort_values(\"mean_test_accuracy\", ascending=False)[[\"param_model__C\", \n",
    "                                                              \"param_model__class_weight\",\n",
    "                                                           \"mean_train_accuracy\",\n",
    "                                                           \"std_train_accuracy\",\n",
    "                                                           \"mean_test_accuracy\", \n",
    "                                                           \"std_test_accuracy\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 13\n",
    "\n",
    "Conduct a random search to find an optimal value for the `k` hyperparameter for a KNN model. Can you find a model that has improved performance vs. default settings?\n",
    "\n",
    "__Hint__\n",
    "- `n_neighbors` takes an int and not a float so to select a random integer you could use `randint` from `scipy.stats`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex13"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# 8. Summary <a id='sum'></a>\n",
    "\n",
    "For the sake of this workshop you are now done - well done! \n",
    "\n",
    "Hopefully you feel we have addressed the aim of getting an overview of how to use some classical classification models.\n",
    "\n",
    "---\n",
    "\n",
    "# 9. Extra Exercises <a id='extra'></a>\n",
    "\n",
    "Here are some extra exercises around improving your models when you outcome classes are highly imballanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "Ngz3taAVjec3"
   },
   "source": [
    "## 9.1. Improving Models with Imballances\n",
    "\n",
    "There are a number of methods available to address imbalances in a dataset when we refine our models, such as:\n",
    "1. Weighting the classes in the model during training, \n",
    "2. Changing the training metric, \n",
    "3. Resampling the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "RaKa2Q6Jjec3"
   },
   "source": [
    "### Weights\n",
    "\n",
    "During model fitting we can assign a larger penalty to wrong predictions on the minority class.\n",
    "\n",
    "The heuristic used for `class_weight=\"balanced\"` in Scikit-Learn (0.23.1) is:\n",
    "\n",
    "$$\n",
    "\\frac{n}{Nc \\times \\sum\\limits^n_{i=1}I(y_i \\in S)},\n",
    "$$\n",
    "\n",
    "where $n$ are the number of samples, $Nc$ the number of classes, $I$ is an indicator function, and $S$ contains the class elements.\n",
    "\n",
    "We've been using this so far, and it doesn't seem to be doing much for us, so lets look at some other options."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "plVHhxDvjec4"
   },
   "source": [
    "### Changing Training/Validation Metric\n",
    "\n",
    "__Optimising for Accuracy__ \n",
    "\n",
    "During hyperparamter cross-validation the default is to choose the model with the best __overall accuracy__. \n",
    "\n",
    "This gives us a model with the smallest possible total number of misclassified observations, irrespective of which class the errors come from$^5$. \n",
    "\n",
    "ML algorithms typically optimize a reward or cost function computed as a sum over the training examples, the decision rule is likely going to be biased toward the majority class$^9$.\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- _\"In other words, the algorithm implicitly learns a model that optimizes the predictions based on the most abundant class in the dataset, in order to minimize the cost or  maximize the reward during training.\"_<sup>9</sup>. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "53D2HcEQjec4",
    "outputId": "150676a5-436a-4b04-b280-297b4d31e5c4"
   },
   "outputs": [],
   "source": [
    "display(pd.DataFrame(classification_report(y_val, \n",
    "                                   log_rs.predict(X_val), \n",
    "                                   labels=None, \n",
    "                                   target_names=list(LE.classes_), \n",
    "                                   sample_weight=None, \n",
    "                                   digits=2, \n",
    "                                   output_dict=True)).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "LXJNCQ4rjec4"
   },
   "source": [
    "Changing the metric for what is defined as the _\"best model\"_ can help us prioritise models that make particular errors.\n",
    "\n",
    "For example, a credit card company might particularly wish to avoid incorrectly classifying an individual who will default, whereas incorrectly classifying an individual who will not default, though still to be avoided, is less problematic. \n",
    "\n",
    "In this case, __recall__ would therefore be a useful metric to use.\n",
    "\n",
    "Rather than run another cross-validation again, provided that in `scoring` you used a list that contained \"recall\", we can just use our results data to pick the model with the best \"recall\" instead of accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8JjNmHvXjec4",
    "outputId": "4ef18b10-7438-4c40-cdab-bc36ac0a9b43"
   },
   "outputs": [],
   "source": [
    "log_rs_df.sort_values(\"mean_test_recall\", ascending=False)[[\"param_model__class_weight\", \n",
    "                                                                  \"param_model__C\", \n",
    "                                                                  \"mean_test_recall\", \n",
    "                                                                  \"std_test_recall\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "l8DcmIfEjec4"
   },
   "source": [
    "Now we see that `balanced` models are indeed better if we want a good average recall.\n",
    "\n",
    "Lets use the output scores to our advantage and build a function that sets the parameters to the best for that particular metric and then fits our model using the full training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "upqmGHZ_jec4"
   },
   "outputs": [],
   "source": [
    "from sklearn.base import clone\n",
    "\n",
    "# uses the gridsearch or randomsearch object to refit a model\n",
    "# on all the data.\n",
    "def manual_refit(input_model, X, y, gs, metric, disp_df=[]):\n",
    "    output_model = clone(input_model)\n",
    "    \n",
    "    gs_df = pd.DataFrame(gs.cv_results_).sort_values(\"mean_test_\"+metric, ascending=False)\n",
    "    \n",
    "    if disp_df:\n",
    "        display(gs_df[disp_df].head())\n",
    "    \n",
    "    params = gs_df[\"params\"].iloc[0]\n",
    "    output_model = output_model.set_params(**params)\n",
    "    output_model = output_model.fit(X, y)\n",
    "    \n",
    "    return output_model\n",
    "\n",
    "rec_model = manual_refit(log_pipe, X_train, y_train, log_rs, \"recall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lapl-XTdjec5"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 14 (Extra)\n",
    "\n",
    "Compare the model above (`rec_model`) to the one with the best accuracy on the validation set. How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "WMdQ_gVdjec5"
   },
   "source": [
    "### Resampling\n",
    "\n",
    "We can change the distribution of the classes in our training data so the model is less affected by the fact there are more of one class (this tends to work well when the class imballance is particularly large like this case!).\n",
    "\n",
    "#### Under-Sampling\n",
    "\n",
    "A fast way to balance the data is just to randomly select a subset of the data for each class so they have the number of datapoints found in the smallest class.\n",
    "\n",
    "__Notes__\n",
    "\n",
    "- `RandomUnderSampler` is part of the Imblearn package, which allows for a lot of techniques for working with imballanced data.\n",
    "- There is a `resample` method in scikit-learn but Imblearn is a bit smoother to work with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A5qBpfX-jec5",
    "outputId": "d6a4a6fc-4cbd-4e40-9f02-6b0e8941e641"
   },
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "\n",
    "X_train_down, y_train_down = RandomUnderSampler().fit_resample(X_train,\n",
    "                                                  y_train)\n",
    "\n",
    "pd.Series(y_train_down).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZgEbA_1pjec6"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 15 (Extra)\n",
    "\n",
    "Plot the data using a scatterplot before and after undersampling the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "20-8SOrsjec6"
   },
   "source": [
    "---\n",
    "If you want to use a sampler in a model pipeline then you can use the pipeline from `imblearn`. Using a sampler in a pipeline ensure you wont be training __and__ validating your data on a smaller/larger sample than normal and get unrepresentative results! \n",
    "\n",
    "__Note__\n",
    "- This is an easy mistake to make... I did it when setting this up :')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "Pa8blBDVjec6",
    "outputId": "9e81b9e2-246f-4048-ecd2-8ec11125b422"
   },
   "outputs": [],
   "source": [
    "from imblearn.pipeline import Pipeline as ImPipeline\n",
    "\n",
    "log_pipe = ImPipeline([\n",
    "    (\"scaler\", StandardScaler()),\n",
    "    (\"sampler\", RandomUnderSampler(random_state=123)),\n",
    "    (\"model\", LogisticRegression(random_state=42))])\n",
    "    \n",
    "# specify parameters and distributions to sample from\n",
    "log_param_dist = {'model__C':uniform(0.01, 1)}\n",
    "\n",
    "us_log_rs = RandomizedSearchCV(log_pipe, \n",
    "                            param_distributions=log_param_dist,\n",
    "                            n_iter=60, \n",
    "                            scoring = [\"accuracy\", \"f1\",\"recall\",\"precision\"], \n",
    "                            cv=StratifiedKFold(n_splits=5),\n",
    "                            refit=\"recall\", \n",
    "                            random_state=42,\n",
    "                            return_train_score=True)\n",
    "\n",
    "us_log_rs.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NdzVhFDNjec6",
    "outputId": "592b338e-1304-4a0a-cf63-b58a16dcfcdb"
   },
   "outputs": [],
   "source": [
    "us_log_rs_df = pd.DataFrame(us_log_rs.cv_results_)\n",
    "us_log_rs_df.sort_values(\"mean_test_recall\", ascending=False)[[\"param_model__C\", \n",
    "                                                               \"mean_test_recall\", \n",
    "                                                               \"std_test_recall\"]].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "1xk6k_W-jec6"
   },
   "source": [
    "#### Oversampling\n",
    "Data can be oversampled easily by randomly sampling from minority classes with replacement to duplicate original samples. \n",
    "\n",
    "__Notes__\n",
    "- make sure to oversample after splitting the training and validation sets or you may \"bleed\" information into the validation sets of the model when trying to test a model<sup>7</sup>... In-other-words, make sure it is in a pipeline!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JKh7QZK5jec7"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 16 (Extra)\n",
    "\n",
    "Visualise the effects of oversampling and then conduct a cross-validation search for optimal parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "wBlVvcLAjec7"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 17 (Extra)\n",
    "\n",
    "Have a look at the other resampling methods available in `imblearn`. What are they useful for and how do they affect the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "SGeEMzH5jec7"
   },
   "source": [
    "---\n",
    "\n",
    "### 🚩 Exercise 18 (Extra)\n",
    "\n",
    "Conduct a hyperparameter search to see if a logistic regression or knn model can provide better recall using different methods to account for the imballance (Weights, Resampling).\n",
    "\n",
    "- Then examine their performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%run ../scripts/show_solutions.py 07_ex18"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hideCode": false,
    "hidePrompt": false,
    "id": "b3Npc_1vjec9"
   },
   "source": [
    "# References\n",
    "\n",
    "__TODO__\n",
    "- go through these and find the ones I ended up using\n",
    "\n",
    "1. https://scikit-learn.org/stable/datasets/toy_dataset.html\n",
    "2. Warwick J Nash, Tracy L Sellers, Simon R Talbot, Andrew J Cawthorn and Wes B Ford (1994) \"The Population Biology of Abalone (_Haliotis_ species) in Tasmania. I. Blacklip Abalone (_H. rubra_) from the North Coast and Islands of Bass Strait\", Sea Fisheries Division, Technical Report No. 48 (ISSN 1034-3288)\n",
    "3. Géron, A. (2017). Hands-on machine learning with Scikit-Learn and TensorFlow: concepts, tools, and techniques to build intelligent systems. \" O'Reilly Media, Inc.\".\n",
    "4. Murphy, K. P. (2012). Machine learning: a probabilistic perspective. MIT press.\n",
    "5. James, Gareth, Daniela Witten, Trevor Hastie, and Robert Tibshirani. An introduction to statistical learning. Vol. 112. New York: springer, 2013.\n",
    "6. http://contrib.scikit-learn.org/imbalanced-learn/stable/introduction.html\n",
    "7. https://beckernick.github.io/oversampling-modeling/\n",
    "8. Mani, I., & Zhang, I. (2003, August). kNN approach to unbalanced data distributions: a case study involving information extraction. In Proceedings of workshop on learning from imbalanced datasets (Vol. 126).\n",
    "9. Raschka, Sebastian, and Vahid Mirjalili. Python Machine Learning, 2nd Ed. Packt Publishing, 2017.\n",
    "10. Lemaître, G., Nogueira, F., & Aridas, C. K. (2017). Imbalanced-learn: A python toolbox to tackle the curse of imbalanced datasets in machine learning. The Journal of Machine Learning Research, 18(1), 559-563.\n",
    "11. Wilson, D. L. (1972). Asymptotic properties of nearest neighbor rules using edited data. IEEE Transactions on Systems, Man, and Cybernetics, (3), 408-421.\n",
    "12. Tomek, I. (1976). Two modifications of CNN. IEEE Trans. Systems, Man and Cybernetics, 6, 769-772.\n",
    "13. Batista, G. E., Prati, R. C., & Monard, M. C. (2004). A study of the behavior of several methods for balancing machine learning training data. ACM SIGKDD explorations newsletter, 6(1), 20-29.\n",
    "14. https://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html\n",
    "15. Burges, C. J. (1998). A tutorial on support vector machines for pattern recognition. Data mining and knowledge discovery, 2(2), 121-167.\n",
    "16. Müller, A. C., & Guido, S. (2016). Introduction to machine learning with Python: a guide for data scientists. \" O'Reilly Media, Inc.\".\n",
    "17. https://bradleyboehmke.github.io/HOML/svm.html"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Clustering.ipynb",
   "provenance": []
  },
  "hide_code_all_hidden": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
